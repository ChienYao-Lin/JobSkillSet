{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a942fe32",
   "metadata": {},
   "source": [
    "# Data Preparation - Get Data and Label it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff12d3a",
   "metadata": {},
   "source": [
    "## Import Package for Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6471899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, os, spacy, unicodedata, json\n",
    "from spacy import displacy\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1165bba2",
   "metadata": {},
   "source": [
    "## Scrap data\n",
    "I scrape [SEEK](https://www.seek.com.au) to extract the information of recruitment advertisement for data analyst, data scientist and data engineer. There are two steps in this part.\n",
    "1. Get Job Urls: parse the search pages to get the url of each job advertisement.\n",
    "2. Get Job Description: extract the text content for all jobs from the urls I scraped before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac6bce83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:24<00:00,  2.41s/it]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:23<00:00,  2.38s/it]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:23<00:00,  2.37s/it]\n",
      "100%|█████████████████████████████████████████| 542/542 [07:07<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "def getUrlsSeek(keyWord, page):\n",
    "    keyString = keyWord.replace(' ', '-')\n",
    "    mainUrl = 'https://www.seek.com.au'\n",
    "    jobUrlList = []\n",
    "    for i in tqdm(range(1, 1+page)):\n",
    "        time.sleep(1)\n",
    "        url = f\"{mainUrl}/{keyString}-jobs?page={i}\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        jobs = soup.find_all(\"article\")\n",
    "        for job in jobs:\n",
    "            jobid = job[\"data-job-id\"]\n",
    "            jobUrlList.append(f\"https://www.seek.com.au/job/{jobid}\")\n",
    "        time.sleep(1)\n",
    "    return jobUrlList\n",
    "\n",
    "def getContent(urlList):\n",
    "    tags = [\"li\", \"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]\n",
    "    text = []\n",
    "    for url in tqdm(urlList):\n",
    "        time.sleep(0.5)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        details = soup.find(\"div\", {\"data-automation\": \"jobAdDetails\"})\n",
    "        for tag in tags:\n",
    "            desc_all_tag = details.find_all(tag)\n",
    "            text = [*text, *[desc_tag.text for desc_tag in desc_all_tag]]\n",
    "    return list(set(text))\n",
    "\n",
    "\n",
    "job_title = [\"data analyst\", \"data scientist\", \"data engineer\"]\n",
    "urlList = []\n",
    "for job in job_title:\n",
    "    urlList = [*urlList, *getUrlsSeek(job, page = 10)]\n",
    "urlList = list(set(urlList))\n",
    "text = getContent(urlList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26629b6",
   "metadata": {},
   "source": [
    "## Split text by sentence then write it into text file\n",
    "Here I used spacy 'en_core_web_sm' model to split the description of jobs into sentences and write these sentences decoded by unicodedata into text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9203ecb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 9380/9380 [00:46<00:00, 201.80it/s]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentences = []\n",
    "for t in tqdm(text):\n",
    "    sentences = [*sentences, *[i for i in nlp(t).sents]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea928a29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[We could talk about RSL Queensland’s flexible work arrangements, its fully stocked kitchens and fridges and our Benefits Hub.,\n",
       " But the benefits that set us apart include: ,\n",
       " Work with the leadership team to help drive the future of credit.,\n",
       " Shapelets or motif discovery,\n",
       " \n",
       " Should be able to implement the Data Vault & start schema Framework using redshift,\n",
       " If you encounter technical issues please contact I Work for NSW Support Team on 1800 562 679 (Mon-Fri) or support@jobs.nsw.gov.au]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1948e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"../data/data_ad.txt\"):\n",
    "    os.remove(\"../data/data_ad.txt\")\n",
    "with open(\"../data/data_ad.txt\", \"w\") as f:\n",
    "    for sen in sentences:\n",
    "        f.write(unicodedata.normalize(\"NFKD\", str(sen))+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc89d65",
   "metadata": {},
   "source": [
    "## Create patterns for EntityRuler\n",
    "The skill entity list was created by scraping [AngelList](https://angel.co/)'s skill report, but the page is not available now. Here is how the page looked like. ![](https://i.imgur.com/K9QCrAU.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08af57a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HTML</td>\n",
       "      <td>SKILL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Java</td>\n",
       "      <td>SKILL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Javascript</td>\n",
       "      <td>SKILL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Python</td>\n",
       "      <td>SKILL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CSS</td>\n",
       "      <td>SKILL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Text   Type\n",
       "0        HTML  SKILL\n",
       "1        Java  SKILL\n",
       "2  Javascript  SKILL\n",
       "3      Python  SKILL\n",
       "4         CSS  SKILL"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/entitylist.csv')\n",
    "df = df.dropna()\n",
    "ad_skills_regular = [{\"label\": row[1], \"pattern\": row[0]} for row in zip(df['Text'], df['Type'])]\n",
    "ad_skills_lower = [{\"label\": row[1], \"pattern\": row[0].lower()} for row in zip(df['Text'], df['Type'])]\n",
    "ad_skills = [*ad_skills_regular, *ad_skills_lower]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca0dfba",
   "metadata": {},
   "source": [
    "### The format of one pattern "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75290f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'SKILL', 'pattern': 'HTML'},\n",
       " {'label': 'SKILL', 'pattern': 'Java'},\n",
       " {'label': 'SKILL', 'pattern': 'Javascript'},\n",
       " {'label': 'SKILL', 'pattern': 'Python'},\n",
       " {'label': 'SKILL', 'pattern': 'CSS'},\n",
       " {'label': 'SKILL', 'pattern': 'C++'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_skills[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b625ca2",
   "metadata": {},
   "source": [
    "## Build simple NLP model with EntityRuler and then annotate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a6073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "def generate_rule_based_nlp(patterns):\n",
    "    nlp = English()\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "    ruler.add_patterns(patterns)\n",
    "    return nlp\n",
    "\n",
    "def test_model(model, text):\n",
    "    doc = model(text)\n",
    "    entities = []\n",
    "    results = []\n",
    "    for ent in doc.ents:\n",
    "        entities.append((ent.start_char, ent.end_char, ent.label_))\n",
    "    if len(entities) > 0:\n",
    "        results = [text, {\"entities\" : entities}]\n",
    "    return results\n",
    "\n",
    "\n",
    "nlp = generate_rule_based_nlp(ad_skills)\n",
    "ad_data = []\n",
    "\n",
    "with open(\"../data/data_ad.txt\", \"r\") as f:\n",
    "    data = f.read().splitlines()\n",
    "    for line in data:\n",
    "        result = test_model(nlp, line)\n",
    "        if result:\n",
    "            ad_data = [*ad_data, result]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce14a1",
   "metadata": {},
   "source": [
    "## Test annotation and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83110f0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_text = \"Machine Learning Engineering skills who use advanced techniques such as Deep Learning (GPU accelerated), NLP, Graph ML as well as other predictive modelling methods to identify business opportunities from a variety of data sources\"\n",
    "doc = nlp(test_text)\n",
    "displacy.render(doc, style=\"ent\")\n",
    "nlp.to_disk('../model/entRuler')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8302eadd",
   "metadata": {},
   "source": [
    "## Write processed data into json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e65bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"../data/ad_data_labeled.json\"):\n",
    "    os.remove(\"../data/ad_data_labeled.json\")\n",
    "with open(\"../data/ad_data_labeled.json\", \"w\", encoding = \"utf-8\") as f:\n",
    "    json.dump(ad_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcda0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
